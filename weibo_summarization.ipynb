{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import os, json, codecs\n",
    "from bert4keras.bert import load_pretrained_model\n",
    "from bert4keras.utils import SimpleTokenizer, load_vocab\n",
    "from keras.layers import *\n",
    "from keras import backend as K\n",
    "from keras.callbacks import Callback\n",
    "from keras.optimizers import Adam\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args:0-256-32-16\n"
     ]
    }
   ],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "data_base_dir = \"/search/odin/liuyouyuan/pyproject/data/weibo_source\"\n",
    "article_file = \"train_text.txt\"\n",
    "abstract_file = \"train_label.txt\"\n",
    "vocab_path = \"./data/weibo_vocab.json\"\n",
    "\n",
    "# bert 相关\n",
    "config_path = 'chinese_L-12_H-768_A-12/bert_config.json'\n",
    "checkpoint_path = 'chinese_L-12_H-768_A-12/bert_model.ckpt'\n",
    "dict_path = 'chinese_L-12_H-768_A-12/vocab.txt'\n",
    "\n",
    "min_count = 0\n",
    "max_input_len = 256\n",
    "max_output_len = 32\n",
    "batch_size = 16\n",
    "steps_per_epoch = 1000\n",
    "epochs = 10\n",
    "\n",
    "print(f\"args:{min_count}-{max_input_len}-{max_output_len}-{batch_size}\")\n",
    "model_name = './model/model_{}.weights'.format(min_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text(art_file, abs_file):\n",
    "    with open(art_file, \"r\") as art_f, open(abs_file, \"r\") as abs_f:\n",
    "        for t, s in zip(art_f, abs_f):\n",
    "            if len(s) <= max_output_len:\n",
    "                yield t[:max_input_len], s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab_json(vocab_json, data):\n",
    "    if os.path.exists(vocab_json):\n",
    "        chars_dic = json.load(open(vocab_json, encoding='utf-8'))\n",
    "    else:\n",
    "        chars_dic = {}\n",
    "        for tup in tqdm(data, desc='构建字表中'):\n",
    "            for tex in tup:\n",
    "                for c in tex:\n",
    "                    chars_dic[c] = chars_dic.get(c, 0) + 1\n",
    "        chars_dic = [(i, j) for i, j in chars_dic.items() if j >= min_count]\n",
    "        chars_dic = sorted(chars_dic, key=lambda c: - c[1])\n",
    "        chars_dic = [c[0] for c in chars_dic]\n",
    "        json.dump(\n",
    "            chars_dic,\n",
    "            codecs.open(vocab_json, 'w', encoding='utf-8'),\n",
    "            indent=4,\n",
    "            ensure_ascii=False\n",
    "        )\n",
    "    print(\"构建字表成功：\", vocab_json) \n",
    "    return chars_dic\n",
    "\n",
    "\n",
    "def padding(x):\n",
    "    \"\"\"\n",
    "    padding至batch内的最大长度\n",
    "    \"\"\"\n",
    "    ml = max([len(i) for i in x])\n",
    "    return np.array([i + [0] * (ml - len(i)) for i in x])\n",
    "\n",
    "\n",
    "def data_generator(tokenizer, art_abs_data):\n",
    "    \"\"\"构造输入数据流\"\"\"\n",
    "    while True:\n",
    "        X, Y = [], []\n",
    "        for art, abstract in art_abs_data:\n",
    "            x, y = tokenizer.encode(art, abstract)\n",
    "            X.append(x)\n",
    "            Y.append(y)\n",
    "            if len(X) == batch_size:\n",
    "                X = padding(X)\n",
    "                Y = padding(Y)\n",
    "                yield [X, Y], None\n",
    "                X, Y = [], []\n",
    "\n",
    "def gen_sent(model, tokenizer, s, topk=2):\n",
    "    \"\"\"\n",
    "    beam search解码\n",
    "    每次只保留topk个最优候选结果；如果topk=1，那么就是贪心搜索\n",
    "    \"\"\"\n",
    "    token_ids, segment_ids = tokenizer.encode(s[:max_input_len])\n",
    "    # 候选答案id\n",
    "    target_ids = [[] for _ in range(topk)]\n",
    "    # 候选答案分数\n",
    "    target_scores = [0] * topk\n",
    "    # 强制要求输出不超过max_output_len字\n",
    "    for i in range(max_output_len):\n",
    "        _target_ids = [token_ids + t for t in target_ids]\n",
    "        _segment_ids = [segment_ids + [1] * len(t) for t in target_ids]\n",
    "        # 直接忽略[PAD], [UNK], [CLS]\n",
    "        _probas = model.predict([_target_ids, _segment_ids])[:, -1, 3:]\n",
    "        # 取对数，方便计算\n",
    "        _log_probas = np.log(_probas + 1e-6)\n",
    "        # 每一项选出topk\n",
    "        _topk_arg = _log_probas.argsort(axis=1)[:, -topk:]\n",
    "        _candidate_ids, _candidate_scores = [], []\n",
    "        for j, (ids, sco) in enumerate(zip(target_ids, target_scores)):\n",
    "            # 预测第一个字的时候，输入的topk事实上都是同一个，\n",
    "            # 所以只需要看第一个，不需要遍历后面的。\n",
    "            if i == 0 and j > 0:\n",
    "                continue\n",
    "            for k in _topk_arg[j]:\n",
    "                _candidate_ids.append(ids + [k + 3])\n",
    "                _candidate_scores.append(sco + _log_probas[j][k])\n",
    "        _topk_arg = np.argsort(_candidate_scores)[-topk:]\n",
    "        for j, k in enumerate(_topk_arg):\n",
    "            target_ids[j].append(_candidate_ids[k][-1])\n",
    "            target_scores[j] = _candidate_scores[k]\n",
    "        ends = [j for j, k in enumerate(target_ids) if k[-1] == 3]\n",
    "        if len(ends) > 0:\n",
    "            k = np.argmax([target_scores[j] for j in ends])\n",
    "            return tokenizer.decode(target_ids[ends[k]])\n",
    "    # 如果max_output_len字都找不到结束符，直接返回\n",
    "    return tokenizer.decode(target_ids[np.argmax(target_scores)])\n",
    "\n",
    "\n",
    "def show(model, tokenizer, s_list):\n",
    "    for s in s_list:\n",
    "        print('生成摘要:', gen_sent(model, tokenizer, s))\n",
    "    print()\n",
    "\n",
    "\n",
    "class Evaluate(Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lowest = 1e10\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # 保存最优\n",
    "        if logs['loss'] <= self.lowest:\n",
    "            self.lowest = logs['loss']\n",
    "            model.save_weights(model_name)\n",
    "        # 演示效果\n",
    "        show(model, tokenizer, s_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = '夏天来临，皮肤在强烈紫外线的照射下，晒伤不可避免，因此，晒后及时修复显得尤为重要，否则可能会造成长期伤害。专家表示，选择晒后护肤品要慎重，芦荟凝胶是最安全，有效的一种选择，晒伤严重者，还请及时 就医 。'\n",
    "s2 = '近日，有媒体报道称：章子怡真怀孕了!报道还援引知情人士消息称，“章子怡怀孕大概四五个月，预产期是年底前后，现在已经不接工作了。”这到底是怎么回事?消息是真是假?针对此消息，23日晚8时30分，华西都市报记者迅速联系上了与章子怡家里关系极好的知情人士，这位人士向华西都市报记者证实说：“子怡这次确实怀孕了。她已经36岁了，也该怀孕了。章子怡怀上汪峰的孩子后，子怡的父母亲十分高兴。子怡的母亲，已开始悉心照料女儿了。子怡的预产期大概是今年12月底。”当晚9时，华西都市报记者为了求证章子怡怀孕消息，又电话联系章子怡的亲哥哥章子男，但电话通了，一直没有人接听。有关章子怡怀孕的新闻自从2013年9月份章子怡和汪峰恋情以来，就被传N遍了!不过，时间跨入2015年，事情却发生着微妙的变化。但后据证实，章子怡的“大肚照”只是影片宣传的噱头。后在8月的一天，章子怡和朋友吃饭，在酒店门口被风行工作室拍到了，疑似有孕在身!今年7月11日，汪峰本来在上海要举行演唱会，后来因为台风“灿鸿”取消了。而消息人士称，汪峰原来打算在演唱会上当着章子怡的面宣布重大消息，而且章子怡已经赴上海准备参加演唱会了，怎知遇到台风，只好延期'\n",
    "s3 = '中国数字经济蓬勃发展，正成为创新经济发展方式的新引擎。信息技术为制造业、商业等各行各业有效赋能，推动国家不断前进。全国工商联副主席、正泰集团董事长南存辉说，公司深耕制造业数字化转型升级，努力探索物联网技术与智慧能源的深度融合之路。杭州大搜车集团首席执行官姚军红说，当前传统汽车行业进入深度调整期，流通市场急需变革，以降低车辆消费门槛，提振消费市场信心。大搜车采用人工智能、大数据等技术，建立开放平台，精准对接传统工厂、商家和客户，为消费者带来切实利益。旷视科技联合创始人兼首席执行官印奇说，新一轮互联网科技革命和产业变革加速演进，让从业者深切感受到机遇与挑战并存。印奇表示，旷视始终坚持自主创新，已成为全球为数不多的有自主研发深度学习框架的公司之一，将努力帮助中国企业在人工智能时代来临时，成为引领世界的力量。'\n",
    "s_list = [s1,s2,s3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "构建字表成功： ./data/weibo_vocab.json\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Input-Token (InputLayer)        (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Input-Segment (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Token (Embedding)     (None, None, 768)    4855296     Input-Token[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Segment (Embedding)   (None, None, 768)    1536        Input-Segment[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Token-Segment (Add)   (None, None, 768)    0           Embedding-Token[0][0]            \n",
      "                                                                 Embedding-Segment[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Position (PositionEmb (None, None, 768)    393216      Embedding-Token-Segment[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Dropout (Dropout)     (None, None, 768)    0           Embedding-Position[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Norm (LayerNormalizat (None, None, 768)    1536        Embedding-Dropout[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Input-Mask (Lambda)             (None, None)         0           Input-Token[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Attention-Mask (Lambda)         (None, None, None)   0           Input-Segment[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, None, 768)    2362368     Embedding-Norm[0][0]             \n",
      "                                                                 Embedding-Norm[0][0]             \n",
      "                                                                 Embedding-Norm[0][0]             \n",
      "                                                                 Input-Mask[0][0]                 \n",
      "                                                                 Attention-Mask[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-1-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, None, 768)    0           Embedding-Norm[0][0]             \n",
      "                                                                 Encoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, None, 768)    1536        Encoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward (FeedForw (None, None, 768)    4722432     Encoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Dropout ( (None, None, 768)    0           Encoder-1-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Add (Add) (None, None, 768)    0           Encoder-1-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-1-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Norm (Lay (None, None, 768)    1536        Encoder-1-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-1-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-1-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-1-FeedForward-Norm[0][0] \n",
      "                                                                 Input-Mask[0][0]                 \n",
      "                                                                 Attention-Mask[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-2-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-1-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, None, 768)    1536        Encoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward (FeedForw (None, None, 768)    4722432     Encoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward-Dropout ( (None, None, 768)    0           Encoder-2-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward-Add (Add) (None, None, 768)    0           Encoder-2-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-2-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward-Norm (Lay (None, None, 768)    1536        Encoder-2-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-2-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-2-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-2-FeedForward-Norm[0][0] \n",
      "                                                                 Input-Mask[0][0]                 \n",
      "                                                                 Attention-Mask[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-3-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-2-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-3-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, None, 768)    1536        Encoder-3-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward (FeedForw (None, None, 768)    4722432     Encoder-3-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward-Dropout ( (None, None, 768)    0           Encoder-3-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward-Add (Add) (None, None, 768)    0           Encoder-3-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-3-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward-Norm (Lay (None, None, 768)    1536        Encoder-3-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-3-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-3-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-3-FeedForward-Norm[0][0] \n",
      "                                                                 Input-Mask[0][0]                 \n",
      "                                                                 Attention-Mask[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-4-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-3-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-4-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-MultiHeadSelfAttentio (None, None, 768)    1536        Encoder-4-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-FeedForward (FeedForw (None, None, 768)    4722432     Encoder-4-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-FeedForward-Dropout ( (None, None, 768)    0           Encoder-4-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-FeedForward-Add (Add) (None, None, 768)    0           Encoder-4-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-4-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-FeedForward-Norm (Lay (None, None, 768)    1536        Encoder-4-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-4-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-4-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-4-FeedForward-Norm[0][0] \n",
      "                                                                 Input-Mask[0][0]                 \n",
      "                                                                 Attention-Mask[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-5-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-4-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-5-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-MultiHeadSelfAttentio (None, None, 768)    1536        Encoder-5-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-FeedForward (FeedForw (None, None, 768)    4722432     Encoder-5-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-FeedForward-Dropout ( (None, None, 768)    0           Encoder-5-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-FeedForward-Add (Add) (None, None, 768)    0           Encoder-5-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-5-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-FeedForward-Norm (Lay (None, None, 768)    1536        Encoder-5-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-5-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-5-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-5-FeedForward-Norm[0][0] \n",
      "                                                                 Input-Mask[0][0]                 \n",
      "                                                                 Attention-Mask[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-6-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-5-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-6-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-MultiHeadSelfAttentio (None, None, 768)    1536        Encoder-6-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-FeedForward (FeedForw (None, None, 768)    4722432     Encoder-6-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-FeedForward-Dropout ( (None, None, 768)    0           Encoder-6-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-FeedForward-Add (Add) (None, None, 768)    0           Encoder-6-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-6-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-FeedForward-Norm (Lay (None, None, 768)    1536        Encoder-6-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-6-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-6-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-6-FeedForward-Norm[0][0] \n",
      "                                                                 Input-Mask[0][0]                 \n",
      "                                                                 Attention-Mask[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-7-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-6-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-7-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-MultiHeadSelfAttentio (None, None, 768)    1536        Encoder-7-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-FeedForward (FeedForw (None, None, 768)    4722432     Encoder-7-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-FeedForward-Dropout ( (None, None, 768)    0           Encoder-7-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-FeedForward-Add (Add) (None, None, 768)    0           Encoder-7-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-7-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-FeedForward-Norm (Lay (None, None, 768)    1536        Encoder-7-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-7-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-7-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-7-FeedForward-Norm[0][0] \n",
      "                                                                 Input-Mask[0][0]                 \n",
      "                                                                 Attention-Mask[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-8-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-7-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-8-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-MultiHeadSelfAttentio (None, None, 768)    1536        Encoder-8-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-FeedForward (FeedForw (None, None, 768)    4722432     Encoder-8-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-FeedForward-Dropout ( (None, None, 768)    0           Encoder-8-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-FeedForward-Add (Add) (None, None, 768)    0           Encoder-8-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-8-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-FeedForward-Norm (Lay (None, None, 768)    1536        Encoder-8-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-8-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-8-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-8-FeedForward-Norm[0][0] \n",
      "                                                                 Input-Mask[0][0]                 \n",
      "                                                                 Attention-Mask[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-9-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-8-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-9-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-MultiHeadSelfAttentio (None, None, 768)    1536        Encoder-9-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-FeedForward (FeedForw (None, None, 768)    4722432     Encoder-9-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-FeedForward-Dropout ( (None, None, 768)    0           Encoder-9-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-FeedForward-Add (Add) (None, None, 768)    0           Encoder-9-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-9-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-FeedForward-Norm (Lay (None, None, 768)    1536        Encoder-9-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-MultiHeadSelfAttenti (None, None, 768)    2362368     Encoder-9-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-9-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-9-FeedForward-Norm[0][0] \n",
      "                                                                 Input-Mask[0][0]                 \n",
      "                                                                 Attention-Mask[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-MultiHeadSelfAttenti (None, None, 768)    0           Encoder-10-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-MultiHeadSelfAttenti (None, None, 768)    0           Encoder-9-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-10-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-MultiHeadSelfAttenti (None, None, 768)    1536        Encoder-10-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-FeedForward (FeedFor (None, None, 768)    4722432     Encoder-10-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-FeedForward-Dropout  (None, None, 768)    0           Encoder-10-FeedForward[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-FeedForward-Add (Add (None, None, 768)    0           Encoder-10-MultiHeadSelfAttention\n",
      "                                                                 Encoder-10-FeedForward-Dropout[0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-FeedForward-Norm (La (None, None, 768)    1536        Encoder-10-FeedForward-Add[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-MultiHeadSelfAttenti (None, None, 768)    2362368     Encoder-10-FeedForward-Norm[0][0]\n",
      "                                                                 Encoder-10-FeedForward-Norm[0][0]\n",
      "                                                                 Encoder-10-FeedForward-Norm[0][0]\n",
      "                                                                 Input-Mask[0][0]                 \n",
      "                                                                 Attention-Mask[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-MultiHeadSelfAttenti (None, None, 768)    0           Encoder-11-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-MultiHeadSelfAttenti (None, None, 768)    0           Encoder-10-FeedForward-Norm[0][0]\n",
      "                                                                 Encoder-11-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-MultiHeadSelfAttenti (None, None, 768)    1536        Encoder-11-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-FeedForward (FeedFor (None, None, 768)    4722432     Encoder-11-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-FeedForward-Dropout  (None, None, 768)    0           Encoder-11-FeedForward[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-FeedForward-Add (Add (None, None, 768)    0           Encoder-11-MultiHeadSelfAttention\n",
      "                                                                 Encoder-11-FeedForward-Dropout[0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-FeedForward-Norm (La (None, None, 768)    1536        Encoder-11-FeedForward-Add[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-MultiHeadSelfAttenti (None, None, 768)    2362368     Encoder-11-FeedForward-Norm[0][0]\n",
      "                                                                 Encoder-11-FeedForward-Norm[0][0]\n",
      "                                                                 Encoder-11-FeedForward-Norm[0][0]\n",
      "                                                                 Input-Mask[0][0]                 \n",
      "                                                                 Attention-Mask[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-MultiHeadSelfAttenti (None, None, 768)    0           Encoder-12-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-MultiHeadSelfAttenti (None, None, 768)    0           Encoder-11-FeedForward-Norm[0][0]\n",
      "                                                                 Encoder-12-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-MultiHeadSelfAttenti (None, None, 768)    1536        Encoder-12-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-FeedForward (FeedFor (None, None, 768)    4722432     Encoder-12-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-FeedForward-Dropout  (None, None, 768)    0           Encoder-12-FeedForward[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-FeedForward-Add (Add (None, None, 768)    0           Encoder-12-MultiHeadSelfAttention\n",
      "                                                                 Encoder-12-FeedForward-Dropout[0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-FeedForward-Norm (La (None, None, 768)    1536        Encoder-12-FeedForward-Add[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "MLM-Dense (Dense)               (None, None, 768)    590592      Encoder-12-FeedForward-Norm[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "MLM-Norm (LayerNormalization)   (None, None, 768)    1536        MLM-Dense[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "MLM-Proba (EmbeddingDense)      (None, None, 6322)   6322        MLM-Norm[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 90,904,498\n",
      "Trainable params: 90,904,498\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 从文件读取文章与参考摘要\n",
    "art_file = os.path.join(data_base_dir, article_file)\n",
    "abs_file = os.path.join(data_base_dir, abstract_file)\n",
    "data = read_text(art_file, abs_file)\n",
    "\n",
    "# 构建自己的字表\n",
    "vocab_chars_dic = build_vocab_json(vocab_path,data)\n",
    "# 读取bert词典\n",
    "_token_dict = load_vocab(dict_path)\n",
    "# 构建新的token_dict 用于构建Tokenizer\n",
    "# keep_words是在bert中保留的字表\n",
    "token_dict, keep_words = {}, []\n",
    "for c in ['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[unused1]']:\n",
    "    token_dict[c] = len(token_dict)\n",
    "    keep_words.append(_token_dict[c])\n",
    "for c in vocab_chars_dic:\n",
    "    if c in _token_dict:\n",
    "        token_dict[c] = len(token_dict)\n",
    "        keep_words.append(_token_dict[c])       \n",
    "# 建立分词器\n",
    "tokenizer = SimpleTokenizer(token_dict)\n",
    "\n",
    "# 定义模型\n",
    "model = load_pretrained_model(\n",
    "    config_path,\n",
    "    checkpoint_path,\n",
    "    seq2seq=True,\n",
    "    keep_words=keep_words,  # 只保留keep_words中的字，精简原字表\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# 交叉熵作为loss，并mask掉输入部分的预测\n",
    "# 目标tokens\n",
    "y_in = model.input[0][:, 1:]\n",
    "y_mask = model.input[1][:, 1:]\n",
    "# 预测tokens，预测与目标错开一位\n",
    "y = model.output[:, :-1]\n",
    "cross_entropy = K.sparse_categorical_crossentropy(y_in, y)\n",
    "cross_entropy = K.sum(cross_entropy * y_mask) / K.sum(y_mask)\n",
    "\n",
    "model.add_loss(cross_entropy)\n",
    "model.compile(optimizer=Adam(1e-5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 244s 244ms/step - loss: 3.2214\n",
      "生成摘要: 晒后护肤品复发理\n",
      "生成摘要: 南京：章子 怡 怀了4？0月 \n",
      "生成摘要: 全国互联网革命：大搜车、大搜车、等等等等\n",
      "\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 220s 220ms/step - loss: 2.5690\n",
      "生成摘要: 护后严重！，芦请就医和就医 \n",
      "生成摘要: [话筒] 消？  子怡怀孕45 月称怀孕章话怡怀 \n",
      "生成摘要: 大搜车集团首席执行官姚军红：大搜车将为消费者带来\n",
      "\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 231s 231ms/step - loss: 2.2497\n",
      "生成摘要: 晒伤护肤品要慎重！\n",
      "生成摘要: 章子怡怀孕了？ \n",
      "生成摘要: 中国互联旷网创旷野 \n",
      "\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 244s 244ms/step - loss: 2.1898\n",
      "生成摘要: 晒后心]] \n",
      "生成摘要: 章子怡怀孕4五个月，章子怡怀孕45个月 \n",
      "生成摘要: 全天，你搜车！ \n",
      "\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 226s 226ms/step - loss: 2.3092\n",
      "生成摘要: 晒后晒肤要慎重！\n",
      "生成摘要: 章子怡怀孕了5怀孕？ \n",
      "生成摘要: 中国数字网济：世界上最各行的深度融合 \n",
      "\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 225s 225ms/step - loss: 2.2491\n",
      "生成摘要: 晒后护肤品慎重 \n",
      "生成摘要: 章子怡怀孕45个月 \n",
      "生成摘要: 全国工商联副主席：大搜车将商慧、智慧网源\n",
      "\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 249s 249ms/step - loss: 2.1415\n",
      "生成摘要: 晒后护肤品芦荟凝胶，效果怎么样？ \n",
      "生成摘要: 章子怡怀孕了5月？ \n",
      "生成摘要: 大国工集家联席执行官：大搜车将加速推进消费车辆、商家 \n",
      "\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 230s 230ms/step - loss: 2.0363\n",
      "生成摘要: 晒后心疼]芦荟凝胶的么？ \n",
      "生成摘要: [话筒]上四五个月，知情人怀工妇女怀\n",
      "生成摘要: [话数]网友：将将需要的创新升\n",
      "\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 244s 244ms/step - loss: 1.7902\n",
      "生成摘要: 晒后护肤品要慎重 \n",
      "生成摘要: 章子怡怀孕45个月 章子怡怀孕45个月 \n",
      "生成摘要: 中国轮字经济革命和新引擎 \n",
      "\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 225s 225ms/step - loss: 1.3050\n",
      "生成摘要: 夏伤及时修要活 \n",
      "生成摘要: 华西都真怀消期息\n",
      "生成摘要: 中国大搜车集团董事长姚军红：将新一轮互联网科技革命和\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # 训练\n",
    "    evaluator = Evaluate()\n",
    "    model.fit_generator(\n",
    "         data_generator(tokenizer, data),\n",
    "         steps_per_epoch=steps_per_epoch,\n",
    "         epochs=epochs,\n",
    "         callbacks=[evaluator]\n",
    "     )\n",
    "    # 预测效果\n",
    "    #model.load_weights(model_name)\n",
    "    #show(model, tokenizer, s_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
